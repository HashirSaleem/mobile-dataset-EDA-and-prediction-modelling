# mobile-dataset-EDA-and-prediction-modelling
# Mobile Price Prediction

This project aims to predict mobile prices based on various features using different machine learning algorithms. The project includes Exploratory Data Analysis
(EDA) and the application of algorithms such as Support Vector Machines (SVM), KMeans clustering, K-Nearest Neighbors (KNN), and more.

## Table of Contents

- [Introduction](#introduction)
- [Dataset](#dataset)
- [Installation](#installation)
- [Exploratory Data Analysis (EDA)](#exploratory-data-analysis-eda)
- [Modeling](#modeling)
  - [Support Vector Machines (SVM)](#support-vector-machines-svm)
  - [KMeans Clustering](#kmeans-clustering)
  - [K-Nearest Neighbors (KNN)](#k-nearest-neighbors-knn)
  - [Other Algorithms](#other-algorithms)
- [Results](#results)

## Introduction

In this project, we use a dataset of mobile features to predict their prices. The goal is to build a model that can accurately classify the price range of mobiles based 
on their features. We perform data cleaning, EDA, and apply multiple machine learning algorithms to achieve this goal.

## Dataset

The dataset used for this project contains various features of mobile phones and their corresponding price ranges. The key features include battery power, clock speed, 
RAM, etc.

Exploratory Data Analysis (EDA)
We perform EDA to understand the distribution of the data, detect outliers, and identify correlations between features. The EDA steps include:

Data Cleaning
Descriptive Statistics
Data Visualization
Modeling
We apply various machine learning algorithms to predict the price range of mobiles. The algorithms include:

Support Vector Machines (SVM)
SVM is used to classify the mobile prices into different categories. We use grid search for hyperparameter tuning.

KMeans Clustering
KMeans clustering is applied to group the mobiles based on their features. This helps in identifying similar groups of mobiles.

K-Nearest Neighbors (KNN)
KNN is used for classification. We explore different values of K to find the best performance.

Other Algorithms
Other algorithms explored include Decision Trees, Random Forest, and Gradient Boosting.

Results
We evaluate the performance of each model using metrics such as accuracy, precision, recall, and F1-score. The results are summarized in a comparative table.






